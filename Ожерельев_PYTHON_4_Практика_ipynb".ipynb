{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esta1d/SF_Data_Science_practice/blob/main/%D0%9E%D0%B6%D0%B5%D1%80%D0%B5%D0%BB%D1%8C%D0%B5%D0%B2_PYTHON_4_%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D0%BA%D0%B0_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center> Практическое задание по теме \"Циклы\""
      ],
      "metadata": {
        "id": "liFg08vGk-Ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=https://1gai.ru/uploads/posts/2020-12/1608804716_54441.png width=300 align=\"right\">\n",
        "\n",
        "→ Поздравляем с освоением важных для анализа данных конструкций Python (переменные, структуры данных, условные операторы и циклы)!\n",
        "\n",
        "Настало время промежуточной практики, чтобы закрепить все полученные в предыдущих модулях навыки! Мы будем применять их на реальном проекте. Сегодня мы обратимся к классике: займемся анализом текстов на примере «Войны и мира» Льва Николаевича Толстого!\n",
        "\n",
        "В рамках практического кейса мы сначала вместе познакомимся с исходными данными и произведем некоторые манипуляции над ними. После чего вам предстоит самостоятельно выполнить несколько заданий на тему поиска наиболее значимых слов в тексте с помощью методов статистического анализа текста. \n",
        "\n",
        "> Итак, приступим!"
      ],
      "metadata": {
        "id": "iZQ8hwHMphX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center> Знакомимся с данными"
      ],
      "metadata": {
        "id": "WgQNPhZ4k68p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Текст произведения мы взяли в библиотеке [lib.ru](http://az.lib.ru/t/tolstoj_lew_nikolaewich/text_0073.shtml) и провели первоначальную обработку. Поскольку наша цель — обработка слов из этого произведения, мы разбили текст на слова и вывели каждое слово в отдельной строке. Кроме того, в местах, где начинаются главы, мы вывели строку `\"[new chapter]\"`.\n",
        "\n",
        "> Исходный текстовый файл хранится в общем доступе и находится [здесь](https://raw.githubusercontent.com/SkillfactoryDS/Datasets/master/war_peace_processed.txt)."
      ],
      "metadata": {
        "id": "Xn4Fncpvk6Va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для начала скачаем текст книги по ссылке."
      ],
      "metadata": {
        "id": "lGItOdWbx4KT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8cSMJp2anzX",
        "outputId": "245fe38b-650a-4f8d-fabe-700563c9ed77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', 'в', 'два', 'раза', 'короче', 'и', 'в', 'пять', 'раз', 'интереснее', '2', 'почти', 'нет', 'философических', 'отступлений', '3', 'в', 'сто', 'раз', 'легче', 'читать', 'весь', 'французский', 'текст', 'заменен', 'русским', 'в', 'переводе', 'самого', 'толстого', '4', 'гораздо', 'больше', 'мира', 'и', 'меньше', 'войны', '5', 'хеппи-энд', 'эти', 'слова', 'я', 'поместил', 'семь', 'лет', 'назад', 'на', 'обложку', 'предыдущего', 'издания', 'указав', 'в', 'аннотации', 'первая', 'полная', 'редакция', 'великого', 'романа', 'созданная', 'к', 'концу', '1866', 'года', 'до', 'того', 'как', 'толстой', 'переделал', 'его', 'в', '1867--1869', 'годах', '--', 'и', 'что', 'я', 'использовал', 'такие-то', 'публикации', 'думая', 'что', 'все', 'всё', 'знают', 'я', 'не', 'объяснил', 'откуда', 'взялась', 'эта', 'первая', 'редакция', 'я', 'оказался', 'неправ', 'и', 'в', 'результате', 'оголтелые', 'и']\n"
          ]
        }
      ],
      "source": [
        "# Импортируем библиотеку для выполнения HTTP-запросов в интернет\n",
        "import requests \n",
        "\n",
        "# Читаем текстовый файл по url-ссылке\n",
        "data = requests.get(\"https://raw.githubusercontent.com/SkillfactoryDS/Datasets/master/war_peace_processed.txt\").text\n",
        "\n",
        "# Предобрабатываем текстовый файл\n",
        "data = data.split('\\n')\n",
        "data.remove('')\n",
        "data = data + ['[new chapter]']\n",
        "\n",
        "# Выводим первые 100 слов из книги\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center> Работаем с данными"
      ],
      "metadata": {
        "id": "eeTg-aKgCwye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для начала найдем общее количество слов и количество уникальных слов в тексте"
      ],
      "metadata": {
        "id": "bpn_8XFRCo7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Превращаем список в множество, удаляя дублирующиеся слова\n",
        "word_set = set(data)\n",
        "# Удаляем из множества слово, символизирующее раздел между главами\n",
        "word_set.discard('[new chapter]')\n",
        "# Выводим результаты\n",
        "print('Общее количество слов: {}'.format(len(data)))\n",
        "print('Общее количество уникальных слов: {}'.format(len(word_set)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPNZVk27wkpM",
        "outputId": "bf80e184-79ab-4431-fd64-39d3b7e606de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Общее количество слов: 300080\n",
            "Общее количество уникальных слов: 38210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте напишем программу, которая посчитает частоту каждого слова. Для этого создадим словарь, ключами которого будут являться слова, а значения - количество вхождений этого слова в текст произведения. Заодно подсчитаем количество глав"
      ],
      "metadata": {
        "id": "ShCV0QIAqCsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализируем пустой словарь\n",
        "word_counts = {}\n",
        "# Инициализируем количество глав\n",
        "count_chapter = 0\n",
        "# Создаем цикл по всем словам из списка слов\n",
        "for word in data:\n",
        "    # Проверяем, что текущее слово - обозначение новой главы\n",
        "    if word == '[new chapter]':\n",
        "        # Если условие выполняется, то увеличиваем количество глав на 1\n",
        "        count_chapter += 1\n",
        "        # Переходим на новую итерацию цикла\n",
        "        continue\n",
        "    # Проверяем, что текущего слова еще нет в словаре слов\n",
        "    if word not in word_counts:\n",
        "        # Если условие выполняется, инициализируем новый ключ 1\n",
        "        word_counts[word] = 1\n",
        "    else:\n",
        "        # В противном случае, увеличиваем количество слов на 1\n",
        "        word_counts[word] += 1\n",
        "\n",
        "# Выводим количество глав\n",
        "print('Количество глав: {}'.format(count_chapter))\n",
        "\n",
        "# Создаем цикл по ключам и их порядковым номерам полученного словаря\n",
        "for i, key in enumerate(word_counts):\n",
        "    # Выводим только первые 10 слов\n",
        "    if i == 10:\n",
        "        break\n",
        "    print(key, word_counts[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzd2YuYZeCf5",
        "outputId": "13c8d9a0-9a25-4fe5-ad9b-839e5c9ef0f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество глав: 171\n",
            "1 10\n",
            "в 6997\n",
            "два 215\n",
            "раза 35\n",
            "короче 3\n",
            "и 14592\n",
            "пять 61\n",
            "раз 296\n",
            "интереснее 4\n",
            "2 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделим все слова на главы. Для этого создадим список, в котором будем хранить списки - слова из определенной главы."
      ],
      "metadata": {
        "id": "WRo9-Xr-rFwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализируем общий список, в котором будем хранить списки слов в каждой главе\n",
        "chapter_data = []\n",
        "# Инициализируем список слов, в котором будет хранить слова одной главы\n",
        "chapter_words = []\n",
        "\n",
        "# Создаем цикл по всем словам из списка\n",
        "for word in data:\n",
        "    # Проверяем, что текущее слово - обозначение новой главы\n",
        "    if word == '[new chapter]':\n",
        "        # Если условие выполняется, добавляем список со словами из главы в общий список\n",
        "        chapter_data.append(chapter_words)\n",
        "        # Обновляем (перезаписываем) список со словами из текущей главы\n",
        "        chapter_words = []\n",
        "    else:\n",
        "        # В противном случае, добавляем текущее слово в список со словами из главы\n",
        "        chapter_words.append(word)\n",
        "\n",
        "# Проверяем, что у нас получилось столько же списков, сколько глав в произведении\n",
        "print('Вложенный список содержит {} внутренних списка'.format(len(chapter_data)))\n",
        "# Выведем первые 100 слов 0-ой главы\n",
        "print(chapter_data[0][:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsdGmz-frFf0",
        "outputId": "0c818d59-fb19-49e2-9701-993cbc388840"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вложенный список содержит 171 внутренних списка\n",
            "['1', 'в', 'два', 'раза', 'короче', 'и', 'в', 'пять', 'раз', 'интереснее', '2', 'почти', 'нет', 'философических', 'отступлений', '3', 'в', 'сто', 'раз', 'легче', 'читать', 'весь', 'французский', 'текст', 'заменен', 'русским', 'в', 'переводе', 'самого', 'толстого', '4', 'гораздо', 'больше', 'мира', 'и', 'меньше', 'войны', '5', 'хеппи-энд', 'эти', 'слова', 'я', 'поместил', 'семь', 'лет', 'назад', 'на', 'обложку', 'предыдущего', 'издания', 'указав', 'в', 'аннотации', 'первая', 'полная', 'редакция', 'великого', 'романа', 'созданная', 'к', 'концу', '1866', 'года', 'до', 'того', 'как', 'толстой', 'переделал', 'его', 'в', '1867--1869', 'годах', '--', 'и', 'что', 'я', 'использовал', 'такие-то', 'публикации', 'думая', 'что', 'все', 'всё', 'знают', 'я', 'не', 'объяснил', 'откуда', 'взялась', 'эта', 'первая', 'редакция', 'я', 'оказался', 'неправ', 'и', 'в', 'результате', 'оголтелые', 'и']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chapter_data[15][100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TM0OaA3pinfQ",
        "outputId": "458a59c0-2948-45f3-dd23-634bb690c700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'в'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подсчитаем, сколько раз каждое слово встречается в каждой из глав"
      ],
      "metadata": {
        "id": "saO-dds9h-Mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализируем список, в котором будем хранить словари\n",
        "chapter_words_count = []\n",
        "\n",
        "# Создаем цикл по элементам внешнего списка со словами\n",
        "for chapter_words in chapter_data:\n",
        "    # Инициализируем пустой словарь, куда будем добавлять результаты\n",
        "    temp = {}\n",
        "    # Создаем цикл по элементам внутреннего списка\n",
        "    for word in chapter_words:\n",
        "        # Проверяем, что текущего слова еще нет в словаре\n",
        "        if word not in temp:\n",
        "            # Если условие выполняется, добавляем ключ в словарь\n",
        "            temp[word] = 1\n",
        "        else:\n",
        "            # В противном случае, увеличиваем количество влождений слова в главу\n",
        "            temp[word] += 1\n",
        "    # Добавляем получившийся словарь в список\n",
        "    chapter_words_count.append(temp)\n",
        "\n",
        "# Выводим результат\n",
        "print(chapter_words_count[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oGOvOHxGt3M",
        "outputId": "49d5dbae-bccc-48bc-dfb3-f25b0e689037"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'автора': 1, 'я': 20, 'пишу': 2, 'до': 1, 'сих': 1, 'пор': 1, 'только': 5, 'о': 2, 'князьях': 1, 'графах': 1, 'министрах': 1, 'сенаторах': 1, 'и': 42, 'их': 3, 'детях': 1, 'боюсь': 1, 'что': 20, 'вперед': 2, 'не': 18, 'будет': 1, 'других': 1, 'лиц': 1, 'в': 10, 'моей': 1, 'истории': 2, 'может': 4, 'быть': 3, 'это': 6, 'нехорошо': 1, 'нравится': 1, 'публике': 1, 'для': 4, 'нее': 1, 'интереснее': 1, 'поучительнее': 1, 'история': 1, 'мужиков': 2, 'купцов': 2, 'семинаристов': 2, 'но': 5, 'со': 1, 'всем': 2, 'моим': 1, 'желанием': 1, 'иметь': 1, 'как': 4, 'можно': 1, 'больше': 1, 'читателей': 1, 'могу': 4, 'угодить': 1, 'такому': 1, 'вкусу': 1, 'по': 4, 'многим': 1, 'причинам': 1, 'во-первых': 1, 'потому': 11, 'памятники': 1, 'того': 3, 'времени': 2, 'котором': 1, 'остались': 1, 'переписке': 1, 'записках': 1, 'людей': 6, 'высшего': 1, 'круга': 2, 'грамотных': 1, 'даже': 1, 'интересные': 1, 'умные': 1, 'рассказы': 1, 'которые': 1, 'мне': 3, 'удалось': 1, 'слышать': 1, 'слышал': 1, 'от': 3, 'же': 3, 'во-вторых': 1, 'жизнь': 3, 'кучеров': 1, 'каторжников': 1, 'меня': 3, 'представляется': 1, 'однообразною': 1, 'скучною': 1, 'все': 3, 'действия': 3, 'этих': 5, 'представляются': 1, 'вытекающими': 1, 'большей': 1, 'частью': 1, 'из': 3, 'одних': 1, 'тех': 1, 'пружин': 2, 'зависти': 1, 'к': 5, 'более': 1, 'счастливым': 1, 'сословиям': 1, 'корыстолюбия': 1, 'материальных': 1, 'страстей': 1, 'ежели': 2, 'вытекают': 1, 'то': 2, 'так': 4, 'застилаются': 1, 'этими': 1, 'побуждениями': 1, 'трудно': 1, 'понимать': 1, 'описывать': 1, 'в-третьих': 1, 'низших': 1, 'сословий': 1, 'менее': 1, 'носит': 1, 'на': 1, 'себе': 1, 'отпечаток': 1, 'в-четвертых': 1, 'некрасива': 1, 'в-пятых': 1, 'никогда': 2, 'мог': 1, 'понять': 3, 'думает': 5, 'будочник': 1, 'стоя': 1, 'у': 2, 'будки': 1, 'чувствует': 1, 'лавочник': 1, 'зазывая': 1, 'купить': 1, 'помочи': 1, 'галстуки': 1, 'семинарист': 1, 'когда': 3, 'его': 2, 'ведут': 1, 'сотый': 1, 'раз': 1, 'сечь': 1, 'розгами': 1, 'тп': 1, 'этого': 2, 'корова': 1, 'ее': 1, 'доят': 1, 'лошадь': 1, 'везет': 1, 'бочку': 1, 'в-шестых': 1, 'наконец': 1, 'знаю': 1, 'самая': 1, 'лучшая': 1, 'причина': 1, 'сам': 1, 'принадлежу': 1, 'высшему': 1, 'сословию': 1, 'обществу': 1, 'люблю': 1, 'мещанин': 1, 'с': 3, 'гордостью': 1, 'говорил': 1, 'пушкин': 1, 'смело': 1, 'говорю': 1, 'аристократ': 5, 'рождению': 1, 'привычкам': 1, 'положению': 1, 'вспоминать': 1, 'предков': 1, '--': 1, 'отцов': 1, 'дедов': 1, 'прадедов': 1, 'моих': 1, 'совестно': 1, 'особенно': 1, 'радостно': 1, 'воспитан': 1, 'детства': 1, 'любви': 2, 'уважении': 1, 'изящному': 2, 'выражающемуся': 1, 'гомере': 1, 'бахе': 1, 'рафаэле': 1, 'всех': 1, 'мелочах': 1, 'жизни': 1, 'чистым': 1, 'рукам': 1, 'красивому': 1, 'платью': 1, 'столу': 1, 'экипажу': 1, 'был': 1, 'счастлив': 1, 'ни': 5, 'отец': 1, 'мой': 2, 'дед': 1, 'знали': 2, 'нужды': 1, 'борьбы': 1, 'между': 1, 'совестью': 1, 'нуждою': 1, 'имели': 1, 'необходимости': 1, 'никому': 1, 'завидовать': 1, 'кланяться': 1, 'потребности': 1, 'образовываться': 1, 'денег': 1, 'положения': 1, 'свете': 1, 'тому': 1, 'подобных': 1, 'испытаний': 1, 'которым': 1, 'подвергаются': 1, 'люди': 1, 'нужде': 1, 'вижу': 2, 'большое': 1, 'счастье': 2, 'благодарю': 1, 'за': 1, 'него': 2, 'бога': 1, 'принадлежит': 1, 'причины': 1, 'отрекаться': 1, 'пользоваться': 1, 'им': 1, 'верить': 1, 'высокий': 1, 'ум': 1, 'тонкий': 1, 'вкус': 1, 'великую': 1, 'честность': 1, 'человека': 1, 'который': 1, 'ковыряет': 1, 'носу': 1, 'пальцем': 1, 'которого': 1, 'душа': 1, 'богом': 1, 'беседует': 1, 'очень': 1, 'глупо': 1, 'преступно': 1, 'дерзко': 1, 'объявляю': 1, 'читателю': 1, 'какой': 1, 'человек': 1, 'чего': 1, 'он': 1, 'ждать': 1, 'еще': 1, 'время': 1, 'закрыть': 1, 'книгу': 1, 'обличить': 1, 'идиота': 1, 'ретрограда': 1, 'аскоченского': 1, 'которому': 1, 'пользуясь': 1, 'этим': 1, 'случаем': 1, 'спешу': 1, 'заявить': 1, 'давно': 1, 'чувствуемое': 1, 'мною': 1, 'искренное': 1, 'глубокое': 1, 'нешуточное': 1, 'уважение': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chapter_words_count[15]['князю']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytU4Um8vjUqi",
        "outputId": "a7679e7d-b97c-4b60-93e2-5256a34d6958"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем цикл по ключам словаря - спискам слов и их порядковым номерам\n",
        "for chapter_number, chapter_dict in enumerate(chapter_words_count):\n",
        "    # Выводим только первые 5 глав\n",
        "    if chapter_number == 5:\n",
        "        break\n",
        "    # Выводим номер главы\n",
        "    print('-' * 40)\n",
        "    print('Chapter: {}'.format(chapter_number))\n",
        "    print('-' * 40)\n",
        "    # Создаем цикл по ключам - словам и их порядковым номерам\n",
        "    for j, word in enumerate(chapter_dict):\n",
        "        # Выводим первые 10 слов из главы\n",
        "        if j == 10:\n",
        "            break\n",
        "        print(word, chapter_dict[word])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBIS01d0tYNi",
        "outputId": "d56308ff-5101-4c64-b707-b7a752166c62"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Chapter: 0\n",
            "----------------------------------------\n",
            "1 1\n",
            "в 37\n",
            "два 3\n",
            "раза 1\n",
            "короче 1\n",
            "и 34\n",
            "пять 1\n",
            "раз 2\n",
            "интереснее 1\n",
            "2 1\n",
            "----------------------------------------\n",
            "Chapter: 1\n",
            "----------------------------------------\n",
            "автора 1\n",
            "я 20\n",
            "пишу 2\n",
            "до 1\n",
            "сих 1\n",
            "пор 1\n",
            "только 5\n",
            "о 2\n",
            "князьях 1\n",
            "графах 1\n",
            "----------------------------------------\n",
            "Chapter: 2\n",
            "----------------------------------------\n",
            "первая 1\n",
            "----------------------------------------\n",
            "Chapter: 3\n",
            "----------------------------------------\n",
            "-- 81\n",
            "ну 5\n",
            "что 44\n",
            "князь 21\n",
            "генуя 1\n",
            "и 94\n",
            "лукка 1\n",
            "стали 1\n",
            "не 57\n",
            "больше 2\n",
            "----------------------------------------\n",
            "Chapter: 4\n",
            "----------------------------------------\n",
            "гостиная 1\n",
            "анны 2\n",
            "павловны 2\n",
            "начала 1\n",
            "понемногу 1\n",
            "наполняться 1\n",
            "приехала 3\n",
            "высшая 1\n",
            "знать 1\n",
            "петербурга 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте резюмировать, что мы с вами уже получили:\n",
        "\n",
        "* `word_set` - множество из всех слов, которые есть в книге\n",
        "\n",
        "* `count_chapter` - количество глав в книге (171)\n",
        "\n",
        "* `word_counts` - словарь, ключами которого являются слова, а значениями - количество вхождений этих слов в книгу\n",
        "\n",
        "* `chapter_data` - список из 171 списка, где элементы вложенных списков - все слова из главы. Каждый список соответствует своей главе\n",
        "\n",
        "* `chapter_words_count` - список из 171 словаря, где ключи - слова, а значения - количество слов в главе. Каждый словарь соответствует своей главе\n",
        "\n",
        "Учтите, что эти данные могут пригодиться вам при выполнении дальнейших заданий.\n",
        "\n",
        "> А теперь к заданиями!"
      ],
      "metadata": {
        "id": "hVkFtTfgiMAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center> Задания для самостоятельного решения"
      ],
      "metadata": {
        "id": "E4Gq7Waelsji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1.\n",
        "\n",
        "Давайте введем понятие частоты употребления отдельного слова в документе (`term frequency`, или `tf`). В нашем случае речь идёт не о документах, а о главах книги (выше мы писали, что в текстовом документе главы разделяются строкой '[new chapter]').\n",
        "\n",
        "Формула для вычисления `term frequency` для слова `word`:\n",
        "$$ tf_{word, chapter} = \\frac {n_{word, chapter}} {n_{chapter}}$$\n",
        "\n",
        "где \n",
        "* ${n_{word, chapter}}$ - сколько раз слово `word` встрачается в главе `chapter`, \n",
        "* $n_{chapter}$ - количество слов в главе `chapter`.\n",
        "\n",
        "\n",
        "Например, слово `\"гостья\"` употребляется в 15-ой главе 10 раз (${n_{word, chapter}}$).(кстати, главы у нас нумеруются с 0). Общее количество слов в тексте 15-ой главы - 1359 ($n_{chapter}$). Тогда:\n",
        "\n",
        "$$ tf_{гостья, 15} = \\frac{10}{1359} \\approx 0.007358$$\n",
        "\n",
        "**Задание:** \n",
        "\n",
        "Напишите программу, которая позволит получать частоту употребления любого заданного слова `target_word` в заданной главе `target_chapter`. \n",
        "\n",
        "**Дополнительное требование:**\n",
        "\n",
        "*Пострайтесь сделать программу максимально обобщенной. То есть желательно рассчитать характеристику `tf` для всех слов из каждой главы, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
        "\n",
        "**Подсказка:**\n",
        "\n",
        "*Для этого вы можете для каждой главы создать словарь, ключами которого являются слова, а значения - частота употребления этого слова в этой главе*\n",
        "\n",
        "**Протестируйте работу программы на нескольких словах и главах.**"
      ],
      "metadata": {
        "id": "h6gX5LrcKrJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_word = 'гостья'\n",
        "target_chapter = 15\n",
        "\n",
        "# Ваш код здесь\n",
        "\n",
        "\n",
        "\n",
        "# ПЕРВАЯ ЧАСТЬ ЗАДАНИЯ\n",
        "\n",
        "# Создаем переменную, в которую будем записывать количество заданных слов\n",
        "chapter_data_count = chapter_data[target_chapter].count(target_word)\n",
        "# Создаем переменную, которая будет считать общее количесво слов в заданной главе\n",
        "chapter_data_len = len(chapter_data[target_chapter])\n",
        "# Cоздаем переменную, которая будет производить расчет частоты употребления отдельного слова в документе\n",
        "tf = chapter_data_count/chapter_data_len\n",
        "print(tf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ВТОРАЯ ЧАСТЬ ЗАДАНИЯ\n",
        "\n",
        "# Создаем пустой словарь, куда будем добавлять готовые словари\n",
        "list_chapter = dict()\n",
        "# Пробегаемся по всем индексам и главам из списка \n",
        "for index, chapter_value_tf in enumerate(chapter_data):\n",
        "  # Создаем словарь и будем заполнять его в виде {'слово': 'частота слова в главе'}\n",
        "  chapter_dict = dict()\n",
        "  # Создаем вложенный цикл по элементам главы\n",
        "  for value in chapter_value_tf:\n",
        "    # Проверяем, что текущего слова еще нет в словаре\n",
        "    if value not in chapter_dict:\n",
        "      # Если условие выполняется, то задаем значение 1\n",
        "      chapter_dict[value] = 1/len(chapter_data[index])\n",
        "    else:\n",
        "      # В противном случае, увеличиваем количество влождений слова в главу и делим на кол-во слов в главе\n",
        "      chapter_dict[value] += 1/len(chapter_data[index])\n",
        "  # Добавляем в основной словарь получившийся словарь \n",
        "  list_chapter[index] = chapter_dict\n",
        "print(list_chapter[15]['гостья'])\n",
        "      "
      ],
      "metadata": {
        "id": "xCs_lzDEPpJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabfeca0-eeea-45bd-af31-84b8a5675be9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.007358351729212656\n",
            "0.007358351729212656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 2.\n",
        "\n",
        "Пришло время познакомиться с понятием `document frequency`.\n",
        "\n",
        "`Document frequency` (для удобства сократим до `df`) — это доля документов, в которых встречается искомое слово. \n",
        "\n",
        "Вычисляется по формуле:\n",
        "\n",
        "$$ df_{word} = \\frac{N_{word}}{N} $$, \n",
        "\n",
        "где \n",
        "* $N_{word}$ - число документов (глав) содержащих слово `word`, \n",
        "* $N$ - общее число документов (глав).\n",
        "\n",
        "Объясним на примере: наш текст состоит из 171 главы ($N$), а слово `\"человек\"` встречается в 115 главах. Тогда:\n",
        "\n",
        "$$ df_{человек} = \\frac{115}{171} \\approx 0.6725$$\n",
        "\n",
        "**Задание:** \n",
        "\n",
        "Напишите программу, которая позволит вычислять document frequency для заданного слова `target_word` и выведить результат на экран.\n",
        "\n",
        "**Дополнительное требование:**\n",
        "\n",
        "*Пострайтесь сделать программу максимально обобщенной. То есть желательно рассчитать характеристику `df` для всех уникальных слов из книги, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
        "\n",
        "**Подсказка:**\n",
        "*Для этого вы можете создать словарь, ключами которого являются слова из книги, а значения - доля документов, содержащих эти слова*\n",
        "\n",
        "**Протестируйте работу программы на нескольких словах** "
      ],
      "metadata": {
        "id": "XyZ5EhoLRbnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_word = 'человек'\n",
        "\n",
        "# Ваш код здесь\n",
        "\n",
        "\n",
        "\n",
        "# ПЕРВАЯ ЧАСТЬ ЗАДАНИЯ\n",
        "\n",
        "# Инициализируем счетчик куда будем записывать, сколько раз нам встретилось слово \"target_word\"\n",
        "count = 0\n",
        "# Находим кол-во глав\n",
        "chapter_data_len = len(chapter_data)\n",
        "# Прбегаемся по всем главам из списка\n",
        "for chapter_list in chapter_data:\n",
        "  # Если слово есть в главе, то увеличиваем счетчик на 1\n",
        "  if target_word in chapter_list:\n",
        "    count += 1\n",
        "# Делим по формуле кол-во число глав содержащих слово \"target_word\" на общее кол-во глав\n",
        "df = count/chapter_data_len\n",
        "print(df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ВТОРАЯ ЧАСТЬ ЗАДАНИЯ\n",
        "\n",
        "# Заполняем множество уникальных слов\n",
        "words_set = set()\n",
        "for chapter in chapter_data:\n",
        "  for word in chapter:\n",
        "    words_set.add(word)\n",
        "\n",
        "# Заполняем словарь в виде {'слово': 'частота по всем главам'}        \n",
        "frequency_dict = dict()\n",
        "# Пробегаемся по каждому уникальному слову\n",
        "for unique_word in words_set:\n",
        "  # Инициализируем счетчик уникального слова по всем главам\n",
        "  word_counter = 0\n",
        "  for chapter in chapter_data:\n",
        "    # Если слово есть в главе, то увеличиваем счетчик на 1\n",
        "    if unique_word in chapter:\n",
        "      word_counter += 1\n",
        "  # Находим частоту слова\n",
        "  frequency_dict[unique_word] = word_counter / len(chapter_data)\n",
        "print(frequency_dict['человек'])\n",
        "  "
      ],
      "metadata": {
        "id": "e8EN3GZ1YCqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18a744c5-da10-4984-8c14-368ca8f5abe6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.672514619883041\n",
            "0.672514619883041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 3\n",
        "\n",
        "Пришло время дать разъяснения: для чего мы делали вычисления выше и что нас ждет впереди?\n",
        "\n",
        "> Если какое-то слово часто употребляется в документе, то, вероятно, этот документ что-то рассказывает о предмете/действии, описываемом этим словом. Скажем, если вы читаете книгу, в которой много раз употребляется слово `\"заяц\"`, то, вероятно, эта книга про зайцев.\n",
        "\n",
        "> Однако, если вы возьмёте слово `\"и\"`, то оно будет встречаться почти в каждой книге много раз. \n",
        "\n",
        "Таким образом, если мы хотим найти наиболее значимые слова в книге, мы, с одной стороны, хотим найти наиболее частые слова, а с другой — убрать те, которые не несут важной информации, так как встречаются везде.\n",
        "\n",
        "Такая задача хорошо решается с помощью `tf-idf` — статистической метрики для оценки важности слова в тексте. Другими словами, `tf-idf` — это «контрастность» слова в документе (насколько оно выделяется среди других слов). \n",
        "\n",
        "Формула для вычисления следующая:\n",
        "\n",
        "`tf-idf = term frequency * inverse document frequency`\n",
        "\n",
        "* `tf` — это частотность термина, которая измеряет, насколько часто термин встречается в документе.\n",
        "\n",
        "* `idf` — это обратная документная частотность термина. Она измеряет непосредственно важность термина во всём множестве документов.\n",
        "\n",
        "Чтобы получить `idf`, необходимо поделить 1 на полученную в Задании 1 документную частоту (`df`):\n",
        "\n",
        "$$idf = \\frac{1}{df}$$\n",
        "\n",
        "Мы будем использовать не сырые значения `idf`, а их логарифмы, то есть $tf * log(idf)$. Сейчас мы не будем заострять внимания на том, почему следует использовать именно логарифм — это долгий разговор. Вернемся к нему, когда будем изучать методы машинного обучения для обработки текстов. Подробнее о `tf-idf` вы можете почитать [здесь](https://translated.turbopages.org/proxy_u/en-ru.ru.15518a02-63e76541-6895b80b-74722d776562/https/www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/).\n",
        "\n",
        "В качестве примера измерим `tf-idf` слова `\"анна\"` в главе 4. Слово `\"анна\"` встречается в указанной главе 7 раз, при этом в 4 главе 1060 слов, всего же слово `\"анна\"` упоминается в 32 главах из 171.\n",
        "\n",
        "Таким образом, `tf-idf` данного слова в данной главе будет равно:\n",
        "\n",
        "$$tf\\_idf_{анна, 4} = tf * log(\\frac{1}{df}) = \\frac{7}{1060} * log(\\frac {171}{32}) \\approx 0.011067$$\n",
        "\n",
        "**Примечание:** здесь используется натуральный логарифм по основанию $e$, однако в общем случае основание логарифма не имеет значения, так как характеристика `tf-idf` используется для сравнения контрастности слов между собой\n",
        "\n",
        "**Задание**:\n",
        "\n",
        "Напишите программу, которая позволяет вычислять значение `tf-idf` для заданного слова `target_word` в заданной главе `target_chapter`.\n",
        "\n",
        "**Дополнительное требование:**\n",
        "\n",
        "*Пострайтесь сделать программу максимально оптимальной. То есть желательно рассчитать характеристику `tf-idf` для всех слов из каждой главы книги, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
        "\n",
        "**Протестируйте работу программы на нескольких словах и главах.**"
      ],
      "metadata": {
        "id": "SS78rqWDYZMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Примечание:**\n",
        "\n",
        "Натуральный логарифм можно вычислить с помощью функции [log](https://pythonim.ru/chisla/funktsiya-log-v-python) из встроенного в Python модуля math:\n"
      ],
      "metadata": {
        "id": "MvdwTW_459uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импортируем функцию log из модуля math:\n",
        "from math import log\n",
        "print(log(120))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX5UCiwj63FD",
        "outputId": "30d55979-ebd1-468a-a260-a4f5df149dfc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.787491742782046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Примечание.** \n",
        "\n",
        "**Модуль (библиотека) в Python** — это любой программный файл, который содержит в себе код, включая функции. В нашем случае math — это встроенный модуль, содержащий функционал для математических вычислений. Подробнее о math вы можете почитать [здесь](https://pythonworld.ru/moduli/modul-math.html). "
      ],
      "metadata": {
        "id": "nk_gYbsJ7ayr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_word = 'анна'\n",
        "target_chapter = 4\n",
        "\n",
        "# Ваш код здесь\n",
        "\n",
        "\n",
        "\n",
        "# ПЕРВАЯ ЧАСТЬ ЗАДАНИЯ\n",
        "\n",
        "# Создаем переменную, в которую будем записывать количество заданных слов\n",
        "chapter_data_count = chapter_data[target_chapter].count(target_word)\n",
        "# Создаем переменную, которая будет считать общее количесво слов в заданной главе\n",
        "chapter_data_len = len(chapter_data[target_chapter])\n",
        "# Cоздаем переменную, которая будет производить расчет частоты употребления отдельного слова в документе\n",
        "tf = chapter_data_count/chapter_data_len\n",
        "\n",
        "# Инициализируем счетчик куда будем записывать, сколько раз нам встретилось слово \"target_word\"\n",
        "count = 0\n",
        "# Находим кол-во глав\n",
        "chapter_data_len = len(chapter_data)\n",
        "# Прбегаемся по всем главам из списка\n",
        "for chapter_list in chapter_data:\n",
        "  # Если слово есть в главе, то увеличиваем счетчик на 1\n",
        "  if target_word in chapter_list:\n",
        "    count += 1\n",
        "# Делим по формуле кол-во число глав содержащих слово \"target_word\" на общее кол-во глав\n",
        "df = count/chapter_data_len\n",
        "# Находим tf_idf по формуле\n",
        "tf_idf = tf * log(1/df)\n",
        "print(tf_idf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ВТОРАЯ ЧАСТЬ ЗАДАНИЯ\n",
        "\n",
        "# Создаем пустой список, куда будем добавлять готовые словари\n",
        "dict_chapter_tf = []\n",
        "# Создаем цикл по всем индексам и главам из списка \n",
        "for index, chapter_value_tf in enumerate(chapter_data):\n",
        "  # Создаем словарь, куду будем помещать ключи словаря(\"cлова\") и значеня (\"будем находить частоту используемого слова в конкретной главе\")\n",
        "  chapter_dict = {}\n",
        "  # Создаем вложенный цикл по элементам\n",
        "  for value in chapter_value_tf:\n",
        "    # Проверяем, что текущего слова еще нет в словаре\n",
        "    if value not in chapter_dict:\n",
        "      # Если условие выполняется, добавляем 1 и делим на кол-во слов в главе\n",
        "      chapter_dict[value] = 1/len(chapter_data[index])\n",
        "    else:\n",
        "      # В противном случае, увеличиваем количество влождений слова в главу и делим на кол-во слов в главе\n",
        "      chapter_dict[value] += 1/len(chapter_data[index])\n",
        "  # Добавляем получившийся словарь в список\n",
        "  dict_chapter_tf.append(chapter_dict)\n",
        "\n",
        "\n",
        "# Заполняем множество уникальных слов\n",
        "words_set = set()\n",
        "for chapter in chapter_data:\n",
        "  for word in chapter:\n",
        "    words_set.add(word)\n",
        "\n",
        "# Заполняем словарь в виде {'слово': 'частота по всем главам'}        \n",
        "frequency_dict = dict()\n",
        "# Пробегаемся по каждому уникальному слову\n",
        "for unique_word in words_set:\n",
        "  # Инициализируем счетчик уникального слова по всем главам\n",
        "  word_counter = 0\n",
        "  for chapter in chapter_data:\n",
        "    # Если слово есть в главе, то увеличиваем счетчик на 1\n",
        "    if unique_word in chapter:\n",
        "      word_counter += 1\n",
        "  # Находим частоту слова\n",
        "  frequency_dict[unique_word] = word_counter / len(chapter_data)\n",
        "\n",
        "# Создаем пустой словарь, куда будем помещать наши главы в виде словарей\n",
        "contrast_list = list()\n",
        "# Пробегаемся по индекса и значениям словаря\n",
        "for index, contrast_value in enumerate(dict_chapter_tf):\n",
        "  # Создаем словарь для главы, куда будем помещать значения\n",
        "  dict_contr = {}\n",
        "  # Пробегаемся по каждому уникальному слову\n",
        "  for contrast_word in words_set:\n",
        "    # Проверяем условие вхожжения уникального слова в словарь(главу)\n",
        "    if contrast_word in contrast_value:\n",
        "      # Если условие выполняется, то находим tf_idf по формуле и заносим ключ и его значение в наш словарь\n",
        "      dict_contr[contrast_word] = contrast_value[contrast_word] * log(1/frequency_dict[contrast_word])\n",
        "  # Добавляем получивший словарь в наш основной словарь\n",
        "  contrast_list.append(dict_contr)\n",
        "print(contrast_list[4]['анна'])\n",
        "\n",
        "    \n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pDB99Uu_2QG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad45a74-4d1e-48dc-bd99-55ac90aaf73a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.011067446769736353\n",
            "0.011067446769736351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 4.\n",
        "\n",
        "Теперь, когда мы умеем вычислять `tf-idf` для каждого слова в главе, мы можем найти те слова, которые являются самыми «контрастными» для данной главы, то есть они могут являться в своём роде заголовком для главы.\n",
        "\n",
        "Например, для главы 3 наиболее значимыми словами будут:\n",
        "\n",
        "`\"анна\", \"павловна\", \"функе\"`\n",
        "\n",
        "**Задание:**\n",
        "\n",
        "Напишите программу, которая позволяет вывести три слова, имеющие самое высокое значение `tf-idf` в заданной главе `target_chapter` в порядке убывания `tf-idf`."
      ],
      "metadata": {
        "id": "k_BS6pPmhHdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_chapter = 3\n",
        "\n",
        "# Ваш код здесь\n",
        "\n",
        "# Выведем на экран значения контрастности этих слов, что бы убедиться что сортировка работает\n",
        "print(contrast_list[3]['анна'])\n",
        "print(contrast_list[3]['павловна'])\n",
        "print(contrast_list[3]['функе'])\n",
        "\n",
        "# Производим сортировку словаря и инвертируем его. Псоле чего делаем срез первых 3-х слов\n",
        "max_contrast = sorted(contrast_list[target_chapter], key=contrast_list[target_chapter].get, reverse=True)[:3]\n",
        "print(max_contrast)\n",
        "\n"
      ],
      "metadata": {
        "id": "2k6BzGFJiIAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e5a1a8-055d-4e38-a42e-e2fa1195c4f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009813990764927085\n",
            "0.014654901328619916\n",
            "0.006948193995273866\n",
            "['павловна', 'анна', 'функе']\n"
          ]
        }
      ]
    }
  ]
}